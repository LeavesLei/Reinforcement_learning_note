### 蒙特卡罗方法
---
![MDP分类](https://pic1.zhimg.com/80/v2-046b7405293b6a6957ac0ab212df4a41_hd.jpg)

- 值函数定义

![值函数定义](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5BG_t%7CS_t%3Ds%5Cright%5D%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%7D%5Cright%5D%5C%5C%5C+%5Cleft%283.2%5Cright%29+%5C%5D)
![动作值函数定义](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%2CA_t%3Da%7D%5Cright%5D%5C%5C%5C+%5Cleft%283.3%5Cright%29+%5C%5D)

- 蒙特卡洛方法利用**经验平均**代替随机变量的期望

#### 利用蒙特卡罗方法求平均值
---
![蒙特卡罗中的经验](https://pic1.zhimg.com/80/v2-0de32a6bd8453cb7e56d434373eee9cc_hd.jpg)
- 第一次访问蒙特卡罗方法

![第一次访问蒙特卡罗方法](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%3D%5Cfrac%7BG_%7B11%7D%5Cleft%28s%5Cright%29%2BG_%7B21%7D%5Cleft%28s%5Cright%29%2B%5Ccdots%7D%7BN%5Cleft%28s%5Cright%29%7D+%5C%5D)

    在计算S处的值函数时，只使用每次实验中第一次访问到状态s的返回值G
- 每次访问蒙特卡罗方法

![每次访问蒙特卡罗方法](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%3D%5Cfrac%7BG_%7B11%7D%5Cleft%28s%5Cright%29%2BG_%7B12%7D%5Cleft%28s%5Cright%29%2B%5Ccdots+%2BG_%7B21%7D%5Cleft%28s%5Cright%29%2B%5Ccdots%7D%7BN%5Cleft%28s%5Cright%29%7D+%5C%5D)

    计算状态s处的值函数时，利用所有访问到状态s时的回报返回值
---
![大数定律](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%5Crightarrow%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5C+as%5C+N%5Cleft%28s%5Cright%29%5Crightarrow%5Cinfty+%5C%5D)

    如何获得充足的经验是无模型强化学习的核心所在
---
- 无模型的策略值方法充分评估策略值函数的前提是每个状态都能被访问到，因此在蒙特卡罗方法中必须采用一定的方法保证每个状态都被访问到

&emsp;&emsp;**探索性初始化**：每个状态都有一定的几率作为初始状态。

---
![探索性初始化蒙特卡罗方法](https://pic3.zhimg.com/80/v2-08b4dbca6cb3464522f7f9bb04c67d66_hd.jpg)

---

- 蒙特卡罗方法又分为on-policy（同策略）和off-policy（异策略）
- on-policy：产生数据的策略与评估和要改善的策略是同一个策略
- off-policy：产生数据的策略与评估和要改善的策略不是同一个策略
---

#### off-policy
- 用π表示用来评估和表示的策略，用μ表示产生样本数据的策略


    异策略可以保证充分的探索性。
- 异策略需要满足的条件

&emsp;行动策略μ产生的行为覆盖或包含目标策略π产生的行为 。



- [ ] **重要性采样**

